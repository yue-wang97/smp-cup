{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv( path + 'train/train_labels.txt',sep=u'|',header=None).dropna(1)\n",
    "train_data.columns = ['uid','sex','age','location']\n",
    "test_data = pd.read_csv(path + 'valid/valid_nolabel.txt',sep=u'|',header=None).dropna(1)\n",
    "test_data.columns = ['uid']\n",
    "total_data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "\n",
    "train_data_info = pd.read_csv( path + 'train/train_info.txt',sep=u'|',header=None).dropna(1)\n",
    "train_data_info.columns = ['uid']\n",
    "train_data_info = train_data_info.drop_duplicates('uid')\n",
    "test_data_info = pd.read_csv(path + 'valid/valid_info.txt',sep=u'|',header=None).dropna(1)\n",
    "test_data_info.columns = ['uid']\n",
    "test_data_info = test_data_info.drop_duplicates('uid')\n",
    "total_data_info = pd.concat([train_data_info,test_data_info],axis=0)\n",
    "\n",
    "\n",
    "links = []\n",
    "for i, line in enumerate(open(path + 'train/train_links.txt',encoding='UTF-8')):\n",
    "    line = line.split()\n",
    "    row = {'uid':int(line[0]),'sum_fans':len(line)-1,'fans':' '.join(line[1:])}\n",
    "    links.append(row)\n",
    "train_data_links = pd.DataFrame(links)\n",
    "train_data_links = train_data_links.drop_duplicates()\n",
    "\n",
    "links = []\n",
    "for i, line in enumerate(open(path + 'valid/valid_links.txt',encoding='UTF-8')):\n",
    "    line = line.split()\n",
    "    row = {'uid':int(line[0]),'sum_fans':len(line)-1,'fans':' '.join(line[1:])}\n",
    "    links.append(row)\n",
    "test_data_links = pd.DataFrame(links)\n",
    "test_data_links = test_data_links.drop_duplicates()\n",
    "\n",
    "total_data_links = pd.concat([train_data_links,test_data_links],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "status = []\n",
    "for i, line in enumerate(open(path + 'train/train_status.txt',encoding='UTF-8')):\n",
    "    l = re.search(',',line).span()[0]\n",
    "    r = re.search(',',line).span()[1]\n",
    "    row = {'uid':int(line[:l]),'sta':line[r:]}\n",
    "    status.append(row)\n",
    "train_data_status = pd.DataFrame(status)\n",
    "\n",
    "status = []\n",
    "for i, line in enumerate(open(path + 'valid/valid_status.txt',encoding='UTF-8')):\n",
    "    l = re.search(',',line).span()[0]\n",
    "    r = re.search(',',line).span()[1]\n",
    "    row = {'uid':int(line[:l]),'sta':line[r:]}\n",
    "    status.append(row)\n",
    "test_data_status = pd.DataFrame(status)\n",
    "\n",
    "total_data_status = pd.concat([train_data_status,test_data_status],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_data = pd.merge(total_data,total_data_info,on='uid',how='left')\n",
    "merge_data = pd.merge(merge_data,total_data_links,on='uid',how='left')\n",
    "merge_data.index = range(len(merge_data))\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "\n",
    "total_data_status['retweet'] = total_data_status.sta.map(lambda s:int(s.split(',')[0]))\n",
    "total_data_status['review'] = total_data_status.sta.map(lambda s:int(s.split(',')[1]))\n",
    "total_data_status['source'] = total_data_status.sta.map(lambda s:s.split(',')[2])\n",
    "total_data_status['time'] = total_data_status.sta.map(lambda s:s.split(',')[3])\n",
    "total_data_status['content'] = total_data_status.sta.map(lambda s:','.join(s.split(',')[4:]))\n",
    "contents = total_data_status.groupby('uid')['content'].agg(lambda lst:' '.join(lst))\n",
    "merge_data['contents'] = merge_data.uid.map(contents)\n",
    "merge_data['sum_content'] = merge_data.uid.map(total_data_status.groupby('uid').size())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_data['max_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('max'))\n",
    "merge_data['max_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('max'))\n",
    "merge_data['min_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('min'))\n",
    "merge_data['min_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('min'))\n",
    "merge_data['median_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('median'))\n",
    "merge_data['median_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('median'))\n",
    "merge_data['mean_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('mean'))\n",
    "merge_data['mean_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('mean'))\n",
    "merge_data['std_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('std'))\n",
    "merge_data['std_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('std'))\n",
    "\n",
    "\n",
    "#locationµÿ«¯”≥…‰¥ ±Ì\n",
    "#location地区映射词表\n",
    "d = {'石家庄': '华北',\n",
    " '子陵庙': '华东',\n",
    " '深圳': '华南',\n",
    " '广州': '华南',\n",
    " '宝安': '华南',\n",
    " '刘庄': '华中',\n",
    " '沙市': '华中',\n",
    " '武汉': '华中',\n",
    " '襄阳': '华中',\n",
    " '安陆': '华中',\n",
    " '荆门': '华中',\n",
    " '西安': '西北',\n",
    " '银川': '西北',\n",
    " '成都': '西南',\n",
    " '绵阳': '西南',\n",
    " '上海': '华东',\n",
    " '云南': '西南',\n",
    " '内蒙古': '华北',\n",
    " '北京': '华北',\n",
    " '台湾': '华东',\n",
    " '吉林': '东北',\n",
    " '四川': '西南',\n",
    " '天津': '华北',\n",
    " '宁夏': '西北',\n",
    " '安徽': '华东',\n",
    " '山东': '华东',\n",
    " '山西': '华北',\n",
    " '辽宁': '东北',\n",
    " '重庆': '西南',\n",
    " '陕西': '西北',\n",
    " '青海': '西北',\n",
    " '香港': '华南',\n",
    " '黑龙江': '东北',\n",
    " '长白': '东北',\n",
    " '丹东': '东北',\n",
    " '大庸桥': '东北',\n",
    " '沈阳': '东北',\n",
    " '大连': '东北',\n",
    " '抚顺': '东北',\n",
    " '石家庄': '华北',\n",
    " '朝阳': '华北',\n",
    " '广东': '华南',\n",
    " '广西': '华南',\n",
    " '新疆': '西北',\n",
    " '江苏': '华东',\n",
    " '江西': '华东',\n",
    " '河北': '华北',\n",
    " '河南': '华中',\n",
    " '浙江': '华东',\n",
    " '海南': '华南',\n",
    " '湖北': '华中',\n",
    " '湖南': '华中',\n",
    " '澳门': '华南',\n",
    " '甘肃': '西北',\n",
    " '福建': '华东',\n",
    " '西藏': '西南',\n",
    " '贵州': '西南',\n",
    "}\n",
    "\n",
    "\n",
    "#将location和age转化成需要提交的范围\n",
    "def trans_loc(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    s = s.split(' ')[0]\n",
    "    if s == 'None':\n",
    "        return '华北'\n",
    "    if s == '海外':\n",
    "        return '境外'\n",
    "    return d[s]\n",
    "\n",
    "\n",
    "def trans_age(age):\n",
    "    if pd.isnull(age):\n",
    "        return age\n",
    "    if age <=1979:\n",
    "        return \"-1979\"\n",
    "    elif age<=1989:\n",
    "        return \"1980-1989\"\n",
    "    else:\n",
    "        return \"1990+\"\n",
    "\n",
    "\n",
    "\n",
    "merge_data['location2'] = merge_data['location'].map(trans_loc)\n",
    "merge_data['age2'] = merge_data['age'].map(trans_age)\n",
    "\n",
    "src_lst = total_data_status.groupby('uid')['source'].agg(lambda lst:' '.join(lst))\n",
    "merge_data['source_content'] = merge_data['uid'].map(src_lst) \n",
    "\n",
    "keys = '|'.join(d.keys())\n",
    "merge_data['source_province'] = merge_data['source_content'].map(lambda s:' '.join(re.findall(keys,s)))\n",
    "merge_data['num_province'] = merge_data['contents'].map(lambda s:' '.join(re.findall(keys,s)))\n",
    "\n",
    "d = defaultdict(lambda :'ø’',d)\n",
    "tokenizer = lambda line: [d[w] for w in line.split(' ')]\n",
    "tfv = TfidfVectorizer(tokenizer=tokenizer,norm=False, use_idf=False, smooth_idf=False, sublinear_tf=False)\n",
    "X_all_sp = tfv.fit_transform(merge_data['num_province'])\n",
    "sum_province = X_all_sp.toarray()\n",
    "for i in range(sum_province.shape[1]):\n",
    "    merge_data['sum_province_%d'%i] = sum_province[:,i]\n",
    "\n",
    "\n",
    "\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.mean([len(s.split(' ')) for s in lst]))\n",
    "merge_data['max_content_len'] = merge_data['uid'].map(length)\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.min([len(s.split(' ')) for s in lst]))\n",
    "merge_data['min_content_len'] = merge_data['uid'].map(length)\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.max([len(s.split(' ')) for s in lst]))\n",
    "merge_data['mean_content_len'] = merge_data['uid'].map(length)\n",
    "\n",
    "#merge_data['name_len'] = merge_data.name.map(lambda s:s if pd.isnull(s) else len(re.sub(r'[\\u4e00-\\u9fff]+','',s)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def num_missing(x):    \n",
    "    return sum(x.isnull())  \n",
    "\n",
    "merge_data['num_missing'] = merge_data.apply(num_missing, axis=1) \n",
    "\n",
    "#rankÃÿ’˜\n",
    "merge_data['rank_sum_content'] = merge_data['sum_content'].rank(method='max')\n",
    "merge_data['rank_sum_fans'] = merge_data['sum_fans'].rank(method='max')\n",
    "merge_data['rank_mean_retweet'] = merge_data['mean_retweet'].rank(method='max')\n",
    "merge_data['rank_mean_review'] = merge_data['mean_review'].rank(method='max')\n",
    "merge_data['rank_num_missing'] = merge_data['num_missing'].rank(method='max')\n",
    "\n",
    "\n",
    "merge_data.to_csv(path+'newfeat/merge_data.csv', header='infer', sep = ',', index = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

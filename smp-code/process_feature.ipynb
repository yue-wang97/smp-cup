{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [uidarea_0, uidarea_1, uidarea_2, uidarea_3, uidarea_4, uidarea_5, uidarea_6, uidarea_7]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import operator\n",
    "from matplotlib import pylab as plt\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---read data---\n",
    "path = '../data/'\n",
    "\n",
    "raw_feat = pd.read_csv(path + 'newfeat/merge_data.csv',sep=',', header = 'infer',encoding='UTF-8')\n",
    "\n",
    "\n",
    "# ---area feature---\n",
    "area = ['东北','华北','华中','华东','西北','西南','华南','境外']\n",
    "\n",
    "loc  = defaultdict(list)\n",
    "loc[0] = ['东北','辽宁','吉林','黑龙江','长白山','北戴河','松花湖','老虎滩','锡林郭勒草原','沈阳故宫',\n",
    "          '沈阳','大连','鞍山','抚顺','本溪','丹东','锦州','营口','阜新','辽阳','盘锦','铁岭','朝阳','葫芦岛','兴城','海城',\n",
    "          '长春','四平','松原','白城','辽源','通化','延边','梅河口','公主岭','长影世纪城','净月潭','伪满','查干湖','松花湖',\n",
    "          '清昭陵','清福陵','星海广场','千山','鸭绿江','本溪水洞' ,'甘井子','大工','东财','辽师','吉大',\n",
    "          '太阳岛','亚布力','中央大街','五大连池','扎龙湿地','镜泊湖', '哈工大','哈工程','黑大','哈医大',\n",
    "          '哈尔滨', '齐齐哈尔','鸡西','鹤岗','双鸭山','大庆','伊春','佳木斯','七台河','牡丹江','黑河','绥化','大兴安岭']\n",
    "          \n",
    "loc[1] = [ '华北','北京','天津','河北','山西','内蒙古','呼和浩特','包头','鄂尔多斯','乌兰察布','冀','黄土高原',\n",
    "          '故宫','颐和园','五台山','平遥古城','云冈石窟','野三坡','白洋淀','晋','北大','清华','南开','天大',\n",
    "          '保定','唐山','太原','大慈阁','北戴河','清东陵','白洋淀','避暑山庄','西柏坡','山海关',\n",
    "         '石家庄','辛集','藁城','晋州','新乐','鹿泉','遵化','迁安','秦皇岛','邯郸','武安','邢台','南宫','沙河','涿州','定州',\n",
    "          '安国','高碑店','张家口','承德','沧州','泊头','任丘','黄骅','河间','廊坊','霸州','三河','衡水','冀州','深州']\n",
    "\n",
    "loc[2] = ['华中','河南','湖北','湖南','中原','少林寺','龙门石窟','嵩山','黄鹤楼','神农架','张家界','衡山',\n",
    "          '郑州','殷墟','天地之中','黄帝故里','白马寺',\n",
    "          '开封','洛阳','南阳','漯河','许昌','三门峡','平顶山','周口','驻马店','新乡','鹤壁','焦作','濮阳',\n",
    "          '安阳','商丘','信阳','济源',\n",
    "         '武汉','黄石','襄樊','十堰','荆州','宜昌','荆门','鄂州','孝感','黄冈','咸宁','随州','恩施',\n",
    "          '仙桃','潜江','天门','神农架','武昌','东湖','武当山','三峡','隆中','汉口',\n",
    "         '张家界','岳麓山','岳阳楼','炎帝陵','凤凰古城','韶山','崀山','东江湖',\n",
    "          '长沙','湘潭','株洲','郴州','衡阳','邵阳','永州','湘西','常德','娄底','益阳','怀化']\n",
    "          \n",
    "\n",
    "loc[3] = [ '华东','上海','江苏','浙江','安徽','江西','福建','山东',\n",
    "         '济南','青岛','南京','苏州','无锡','杭州','宁波','厦门','福州','烟台','合肥','南昌','泉州','温州',\n",
    "         '绍兴','台州','威海','南通','泰安','东营','镇江','济宁','扬州','徐州','泰州','九江','赣州','滨州',\n",
    "          '湖州','嘉兴','临沂','金华','宜春','新余','景德镇','盐城','萍乡','芜湖','连云港','聊城','蚌埠','淮南','马鞍山',\n",
    "          '丽水','衢州','安庆','吉安','鹰潭','漳州','淮安','德州','菏泽','宿迁','舟山','淮北','亳州','淮北','铜陵','黄山',\n",
    "          '阜阳','宿州','滁州','六安','宣城','池州','莆田','龙岩','三明','南平','宁德','抚州','上饶','淄博','枣庄','潍坊',\n",
    "          '日照','莱芜','中山陵','上海外滩','杭州西湖','庐山','鼓浪屿','黄山','趵突泉','外滩','豫园','南京路','人民广场',\n",
    "          '静安寺','新天地','迪士尼乐园','黄浦','卢湾','徐汇','长宁','静安','普陀','闸北','虹口','杨浦','浦东','宝山','嘉定',\n",
    "          '闵行','松江','青浦','金山','南汇','奉贤','崇明']\n",
    "\n",
    "loc[4] = [ '西北','甘肃','青海','宁夏','新疆','维吾尔','内蒙古',\n",
    "          '莫高窟','麦积山','崆峒山','嘉峪关','鸣沙山','月牙泉','甘南','兰州','天水','白银','平凉','庆阳','陇南','定西',\n",
    "          '金昌','武威','张掖','酒泉','嘉峪关','临夏','甘南','西宁','海东','格尔木','德令哈','玉树','天山','高昌古城','吐鲁番',\n",
    "          '喀纳斯湖','博斯腾湖','魔鬼城','乌鲁木齐','克拉玛依','吐鲁番','哈密','库尔勒','阿克苏','阿图什','喀什','和田','伊宁',\n",
    "          '奎屯','塔城','乌苏','阿勒泰','石河子','呼和浩特','包头','通辽','赤峰','乌海','鄂尔多斯','呼伦贝尔','乌兰察布',\n",
    "          '巴彦淖尔','兴安','锡林郭勒','阿拉善','满洲里','二连浩特','乌兰浩特','扎兰屯','牙克石','锡林浩特','霍林郭勒',\n",
    "          '丰镇','根河','额尔古纳','阿尔山']\n",
    "\n",
    "loc[5] = ['西南','云南','贵州','四川','西藏','九寨','黄龙','都江堰','青城山','乐山','峨眉山','阆中古城',\n",
    "          '稻城亚丁','川大','电子科大','成都','重庆','昆明','南宁','贵阳','绵阳','宜宾','南充','攀枝花','大理','遵义',\n",
    "          '桂林','柳州','防城港','钦州','六盘水','自贡','德阳','泸州','玉溪','丽江','重大','云大','西财']\n",
    "\n",
    "loc[6] =['华南','广东','广西','海南','台湾','香港','澳门','广州','韶关','深圳','珠海','汕头','佛山','江门',\n",
    "         '湛江','茂名','肇庆','惠州','梅州','汕尾','河源','阳江','清远','东莞','中山','潮州','揭阳','云浮',\n",
    "         '增城','从化','乐昌','南雄','台山','开平','鹤山','恩平','廉江','雷州','吴川','高州','化州','信宜','高要','四会',\n",
    "         '兴宁','陆丰','阳春','英德','连州','普宁','罗定','南宁','柳州','桂林','梧州','北海','防城港','钦州','贵港','玉林',\n",
    "         '百色','贺州','河池','来宾','崇左','岑溪','东兴','桂平','北流','宜州','合山','凭祥','海口','三亚','五指山','琼海',\n",
    "         '儋州','文昌','万宁','台北','高雄','基隆','台中','台南','新竹','嘉义板桥','宜兰','竹北','桃园','苗栗',\n",
    "         '丰原','彰化','南投','嘉义','太保','云林','斗六','新营','凤山','屏东','台东','花莲','澎湖','马公']\n",
    "\n",
    "loc[7] = ['境外','美国','英国','法国','俄罗斯','加拿大','巴西','澳大利亚','印尼','泰国','马来西亚','新加坡',\n",
    "          '菲律宾','越南','印度','日本']\n",
    "featAreaPd = pd.DataFrame(columns=range(len(area)))\n",
    "featAreaPd.columns = 'area_' + featAreaPd.columns.astype(str)\n",
    "\n",
    "print (featAreaPd)\n",
    "alen = len(area)\n",
    "\n",
    "\n",
    "for content in raw_feat['contents']:\n",
    "    tmpDict  = defaultdict(int)\n",
    "    for word in content.split():\n",
    "        tmpDict[word] = tmpDict[word] + 1\n",
    "    tmpL  = [0] * alen\n",
    "    for i in range(len(area)):\n",
    "        for l in loc[i]:\n",
    "            tmpL[i] = tmpL[i] + tmpDict[l]\n",
    "    featAreaPd = featAreaPd.append(pd.DataFrame(np.array(tmpL).reshape(1,alen), columns = list(featAreaPd.columns)), ignore_index=True)\n",
    "\n",
    "    \n",
    "feat_area = featAreaPd\n",
    "feat_area.to_csv(path+'newfeat/feat_area.csv', header='infer', sep = ',', index = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---time feature---\n",
    "def timePeriod(s):\n",
    "    if ((s >'00:00') & (s<='03:00')):\n",
    "        return 0\n",
    "    elif((s >'03:00') & (s<='06:00')):\n",
    "        return 1\n",
    "    elif((s >'06:00') & (s<='09:00')):\n",
    "        return 2\n",
    "    elif((s >'09:00') & (s<='12:00')):\n",
    "        return 3\n",
    "    elif((s >'12:00') & (s<='15:00')):\n",
    "        return 4\n",
    "    elif((s >'15:00') & (s<='18:00')):\n",
    "        return 5\n",
    "    elif((s >'18:00') & (s<='21:00')):\n",
    "        return 6\n",
    "    elif((s >'21:00') & (s<='24:00')):\n",
    "        return 7\n",
    "\n",
    "train_path = path + 'train/' \n",
    "test_path = path + 'valid/'\n",
    "\n",
    "\n",
    "train_status_lines = open(train_path + 'train_status.txt',encoding='UTF-8').readlines()\n",
    "train_dict = dict()\n",
    "early_flag = 0\n",
    "for train_status in train_status_lines:\n",
    "    r = train_status.split(',')\n",
    "    s = r[4].split()\n",
    "    \n",
    "    if(len(s)==1):\n",
    "        early_flag = 1    \n",
    "    else:    \n",
    "        t = timePeriod(s[1])\n",
    "        if((r[0] in train_dict) == False):\n",
    "            train_dict[r[0]] = defaultdict(int)\n",
    "        train_dict[r[0]][t] = train_dict[r[0]][t] + 1\n",
    "        if(early_flag==1):\n",
    "            train_dict[r[0]][t] = train_dict[r[0]][t] + 1\n",
    "            early_flag = 0\n",
    "train_dict_pd = pd.DataFrame(train_dict).T.reset_index().rename(columns={\"index\": \"uid\"})\n",
    "train_dict_pd = train_dict_pd.drop(None, axis=1)\n",
    "train_dict_pd.columns = 'timePeriod_3hour_' + train_dict_pd.columns.astype(str)\n",
    "train_dict_pd = train_dict_pd.rename(columns={\"timePeriod_3hour_uid\": \"uid\"})\n",
    "train_dict_pd = train_dict_pd.fillna(0)\n",
    "\n",
    "test_status_lines = open(test_path + 'valid_status.txt',encoding='UTF-8').readlines()\n",
    "test_dict = dict()\n",
    "early_flag = 0\n",
    "for test_status in test_status_lines:\n",
    "    r = test_status.split(',')\n",
    "    s = r[4].split()\n",
    "    \n",
    "    if(len(s)==1):\n",
    "        early_flag = 1    \n",
    "    else:    \n",
    "        t = timePeriod(s[1])\n",
    "        if((r[0] in test_dict) == False):\n",
    "            test_dict[r[0]] = defaultdict(int)\n",
    "        test_dict[r[0]][t] = test_dict[r[0]][t] + 1\n",
    "        if(early_flag==1):\n",
    "            test_dict[r[0]][t] = test_dict[r[0]][t] + 1\n",
    "            early_flag = 0\n",
    "test_dict_pd = pd.DataFrame(test_dict).T.reset_index().rename(columns={\"index\": \"uid\"})\n",
    "test_dict_pd = test_dict_pd.drop(None, axis=1)\n",
    "test_dict_pd.columns = 'timePeriod_3hour_' + test_dict_pd.columns.astype(str)\n",
    "test_dict_pd = test_dict_pd.rename(columns={\"timePeriod_3hour_uid\": \"uid\"})\n",
    "test_dict_pd = test_dict_pd.fillna(0)\n",
    "\n",
    "feat_time_3hour = pd.concat([train_dict_pd, test_dict_pd], axis = 0, ignore_index = True )\n",
    "\n",
    "def timePerHour(s):\n",
    "    if ((s >'00:00') & (s<='01:00')):\n",
    "        return 0\n",
    "    elif((s >'01:00') & (s<='02:00')):\n",
    "        return 1\n",
    "    elif((s >'02:00') & (s<='03:00')):\n",
    "        return 2\n",
    "    elif((s >'03:00') & (s<='04:00')):\n",
    "        return 3\n",
    "    elif((s >'04:00') & (s<='05:00')):\n",
    "        return 4\n",
    "    elif((s >'05:00') & (s<='06:00')):\n",
    "        return 5\n",
    "    elif((s >'06:00') & (s<='07:00')):\n",
    "        return 6\n",
    "    elif((s >'07:00') & (s<='08:00')):\n",
    "        return 7\n",
    "    elif((s >'08:00') & (s<='09:00')):\n",
    "        return 8\n",
    "    elif((s >'09:00') & (s<='10:00')):\n",
    "        return 9\n",
    "    elif((s >'10:00') & (s<='11:00')):\n",
    "        return 10\n",
    "    elif((s >'11:00') & (s<='12:00')):\n",
    "        return 11\n",
    "    elif((s >'12:00') & (s<='13:00')):\n",
    "        return 12\n",
    "    elif((s >'13:00') & (s<='14:00')):\n",
    "        return 13\n",
    "    elif((s >'14:00') & (s<='15:00')):\n",
    "        return 14\n",
    "    elif((s >'15:00') & (s<='16:00')):\n",
    "        return 15\n",
    "    elif((s >'16:00') & (s<='17:00')):\n",
    "        return 16\n",
    "    elif((s >'17:00') & (s<='18:00')):\n",
    "        return 17\n",
    "    elif((s >'18:00') & (s<='19:00')):\n",
    "        return 18\n",
    "    elif((s >'19:00') & (s<='20:00')):\n",
    "        return 19\n",
    "    elif((s >'20:00') & (s<='21:00')):\n",
    "        return 20\n",
    "    elif((s >'21:00') & (s<='22:00')):\n",
    "        return 21\n",
    "    elif((s >'22:00') & (s<='23:00')):\n",
    "        return 22\n",
    "    elif((s >'23:00') & (s<='24:00')):\n",
    "        return 23\n",
    "\n",
    "train_path = path + 'train/'\n",
    "test_path = path + 'valid/'\n",
    "\n",
    "\n",
    "train_status_lines = open(train_path + 'train_status.txt',encoding='UTF-8').readlines()\n",
    "train_dict = dict()\n",
    "early_flag = 0\n",
    "for train_status in train_status_lines:\n",
    "    r = train_status.split(',')\n",
    "    s = r[4].split()\n",
    "    \n",
    "    if(len(s)==1):\n",
    "        early_flag = 1    \n",
    "    else:    \n",
    "        t = timePerHour(s[1])\n",
    "        if((r[0] in train_dict) == False):\n",
    "            train_dict[r[0]] = defaultdict(int)\n",
    "        train_dict[r[0]][t] = train_dict[r[0]][t] + 1\n",
    "        if(early_flag==1):\n",
    "            train_dict[r[0]][t] = train_dict[r[0]][t] + 1\n",
    "            early_flag = 0\n",
    "train_dict_pd = pd.DataFrame(train_dict).T.reset_index().rename(columns={\"index\": \"uid\"})\n",
    "train_dict_pd = train_dict_pd.drop(None, axis=1)\n",
    "train_dict_pd.columns = 'timePeriod_1hour_' + train_dict_pd.columns.astype(str)\n",
    "train_dict_pd = train_dict_pd.rename(columns={\"timePeriod_1hour_uid\": \"uid\"})\n",
    "train_dict_pd = train_dict_pd.fillna(0)\n",
    "\n",
    "test_status_lines = open(test_path + 'valid_status.txt',encoding='UTF-8').readlines()\n",
    "test_dict = dict()\n",
    "early_flag = 0\n",
    "for test_status in test_status_lines:\n",
    "    r = test_status.split(',')\n",
    "    s = r[4].split()\n",
    "    \n",
    "    if(len(s)==1):\n",
    "        early_flag = 1    \n",
    "    else:    \n",
    "        t = timePerHour(s[1])\n",
    "        if((r[0] in test_dict) == False):\n",
    "            test_dict[r[0]] = defaultdict(int)\n",
    "        test_dict[r[0]][t] = test_dict[r[0]][t] + 1\n",
    "        if(early_flag==1):\n",
    "            test_dict[r[0]][t] = test_dict[r[0]][t] + 1\n",
    "            early_flag = 0\n",
    "test_dict_pd = pd.DataFrame(test_dict).T.reset_index().rename(columns={\"index\": \"uid\"})\n",
    "test_dict_pd = test_dict_pd.drop(None, axis=1)\n",
    "test_dict_pd.columns = 'timePeriod_1hour_' + test_dict_pd.columns.astype(str)\n",
    "test_dict_pd = test_dict_pd.rename(columns={\"timePeriod_1hour_uid\": \"uid\"})\n",
    "test_dict_pd = test_dict_pd.fillna(0)\n",
    "\n",
    "feat_time_1hour = pd.concat([train_dict_pd, test_dict_pd], axis = 0, ignore_index = True )\n",
    "\n",
    "\n",
    "# ---save time feature---\n",
    "feat_time_3hour.to_csv(path+'newfeat/feat_time_3hour.csv', header='infer', sep = ',', index = 0)\n",
    "feat_time_1hour.to_csv(path+'newfeat/feat_time_1hour.csv', header='infer', sep = ',', index = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"newmerge_feat1 = pd.read_csv('../data/newfeat/newmerge_feat.csv',encoding='UTF-8')\\nmerge_data = pd.merge(merge_data,newmerge_feat1,on='uid',how='left')\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sex final result\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "#读入训练测试数据，合并\n",
    "train_data = pd.read_csv('../data/train/train_labels.txt',sep=u'|',header=None).dropna(1)\n",
    "train_data.columns = ['uid','sex','age','location']\n",
    "test_data = pd.read_csv('../data/valid/valid_nolabel.txt',sep=u'|',header=None).dropna(1)\n",
    "test_data.columns = ['uid']\n",
    "total_data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#读入训练测试info数据，合并\n",
    "train_data_info = pd.read_csv('../data/train/train_info.txt',sep=u'|',header=None).dropna(1)\n",
    "train_data_info.columns = ['uid']\n",
    "train_data_info = train_data_info.drop_duplicates()\n",
    "test_data_info = pd.read_csv('../data/valid/valid_info.txt',sep=u'|',header=None).dropna(1)\n",
    "test_data_info.columns = ['uid']\n",
    "test_data_info = test_data_info.drop_duplicates()\n",
    "total_data_info = pd.concat([train_data_info,test_data_info],axis=0)\n",
    "total_data_info = total_data_info.drop_duplicates('uid')\n",
    "\n",
    "\n",
    "\n",
    "#读入训练测试links数据，合并\n",
    "links = []\n",
    "for i, line in enumerate(open('../data/train/train_links.txt',encoding='UTF-8')):\n",
    "    line = line.split()\n",
    "    row = {'uid':int(line[0]),'sum_fans':len(line)-1,'fans':' '.join(line[1:])}\n",
    "    links.append(row)\n",
    "train_data_links = pd.DataFrame(links)\n",
    "train_data_links = train_data_links.drop_duplicates()\n",
    "\n",
    "\n",
    "links = []\n",
    "for i, line in enumerate(open('../data/valid/valid_links.txt',encoding='UTF-8')):\n",
    "    line = line.split()\n",
    "    row = {'uid':int(line[0]),'sum_fans':len(line)-1,'fans':' '.join(line[1:])}\n",
    "    links.append(row)\n",
    "test_data_links = pd.DataFrame(links)\n",
    "test_data_links = test_data_links.drop_duplicates()\n",
    "\n",
    "total_data_links = pd.concat([train_data_links,test_data_links],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#读入训练测试status数据，合并\n",
    "status = []\n",
    "for i, line in enumerate(open('../data/train/train_status.txt',encoding='UTF-8')):\n",
    "    \n",
    "    l = re.search(',',line).span()[0]\n",
    "    r = re.search(',',line).span()[1]\n",
    "    row = {'uid':int(line[:l]),'sta':line[r:]}\n",
    "    status.append(row)\n",
    "train_data_status = pd.DataFrame(status)\n",
    "\n",
    "\n",
    "status = []\n",
    "for i, line in enumerate(open('../data/valid/valid_status.txt',encoding='UTF-8')):\n",
    "    \n",
    "    l = re.search(',',line).span()[0]\n",
    "    r = re.search(',',line).span()[1]\n",
    "    row = {'uid':int(line[:l]),'sta':line[r:]}\n",
    "    status.append(row)\n",
    "test_data_status = pd.DataFrame(status)\n",
    "\n",
    "total_data_status = pd.concat([train_data_status,test_data_status],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#合并题目给的几个表数据\n",
    "merge_data = pd.merge(total_data,total_data_info,on='uid',how='left')\n",
    "merge_data = pd.merge(merge_data,total_data_links,on='uid',how='left')\n",
    "merge_data.index = range(len(merge_data))\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "\n",
    "total_data_status['retweet'] = total_data_status.sta.map(lambda s:int(s.split(',')[0]))\n",
    "total_data_status['review'] = total_data_status.sta.map(lambda s:int(s.split(',')[1]))\n",
    "total_data_status['source'] = total_data_status.sta.map(lambda s:s.split(',')[2])\n",
    "total_data_status['time'] = total_data_status.sta.map(lambda s:s.split(',')[3])\n",
    "total_data_status['content'] = total_data_status.sta.map(lambda s:','.join(s.split(',')[4:]))\n",
    "contents = total_data_status.groupby('uid')['content'].agg(lambda lst:' '.join(lst))\n",
    "merge_data['contents'] = merge_data.uid.map(contents)\n",
    "merge_data['sum_content'] = merge_data.uid.map(total_data_status.groupby('uid').size())\n",
    "\n",
    "\n",
    "\n",
    "#统计特征\n",
    "merge_data['max_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('max'))\n",
    "merge_data['max_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('max'))\n",
    "merge_data['min_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('min'))\n",
    "merge_data['min_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('min'))\n",
    "merge_data['median_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('median'))\n",
    "merge_data['median_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('median'))\n",
    "merge_data['mean_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('mean'))\n",
    "merge_data['mean_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('mean'))\n",
    "merge_data['std_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('std'))\n",
    "merge_data['std_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('std'))\n",
    "\n",
    "\n",
    "#location地区映射词表\n",
    "d = {'石家庄': '华北',\n",
    " '子陵庙': '华东',\n",
    " '深圳': '华南',\n",
    " '广州': '华南',\n",
    " '宝安': '华南',\n",
    " '刘庄': '华中',\n",
    " '沙市': '华中',\n",
    " '武汉': '华中',\n",
    " '襄阳': '华中',\n",
    " '安陆': '华中',\n",
    " '荆门': '华中',\n",
    " '西安': '西北',\n",
    " '银川': '西北',\n",
    " '成都': '西南',\n",
    " '绵阳': '西南',\n",
    " '上海': '华东',\n",
    " '云南': '西南',\n",
    " '内蒙古': '华北',\n",
    " '北京': '华北',\n",
    " '台湾': '华东',\n",
    " '吉林': '东北',\n",
    " '四川': '西南',\n",
    " '天津': '华北',\n",
    " '宁夏': '西北',\n",
    " '安徽': '华东',\n",
    " '山东': '华东',\n",
    " '山西': '华北',\n",
    " '辽宁': '东北',\n",
    " '重庆': '西南',\n",
    " '陕西': '西北',\n",
    " '青海': '西北',\n",
    " '香港': '华南',\n",
    " '黑龙江': '东北',\n",
    " '长白': '东北',\n",
    " '丹东': '东北',\n",
    " '大庸桥': '东北',\n",
    " '沈阳': '东北',\n",
    " '大连': '东北',\n",
    " '抚顺': '东北',\n",
    " '石家庄': '华北',\n",
    " '朝阳': '华北',\n",
    " '广东': '华南',\n",
    " '广西': '华南',\n",
    " '新疆': '西北',\n",
    " '江苏': '华东',\n",
    " '江西': '华东',\n",
    " '河北': '华北',\n",
    " '河南': '华中',\n",
    " '浙江': '华东',\n",
    " '海南': '华南',\n",
    " '湖北': '华中',\n",
    " '湖南': '华中',\n",
    " '澳门': '华南',\n",
    " '甘肃': '西北',\n",
    " '福建': '华东',\n",
    " '西藏': '西南',\n",
    " '贵州': '西南',\n",
    "}\n",
    "\n",
    "\n",
    "#将location和age转化成需要提交的范围\n",
    "def trans_loc(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    s = s.split(' ')[0]\n",
    "    if s == 'None':\n",
    "        return '华北'\n",
    "    if s == '海外':\n",
    "        return s\n",
    "    return d[s]\n",
    "\n",
    "def trans_age(age):\n",
    "    if pd.isnull(age):\n",
    "        return age\n",
    "    if age <=1979:\n",
    "        return \"-1979\"\n",
    "    elif age<=1989:\n",
    "        return \"1980-1989\"\n",
    "    else:\n",
    "        return \"1990+\"\n",
    "\n",
    "\n",
    "\n",
    "merge_data['location2'] = merge_data['location'].map(trans_loc)\n",
    "merge_data['age2'] = merge_data['age'].map(trans_age)\n",
    "\n",
    "src_lst = total_data_status.groupby('uid')['source'].agg(lambda lst:' '.join(lst))\n",
    "merge_data['source_content'] = merge_data['uid'].map(src_lst) \n",
    "\n",
    "keys = '|'.join(d.keys())\n",
    "merge_data['source_province'] = merge_data['source_content'].map(lambda s:' '.join(re.findall(keys,s)))\n",
    "merge_data['num_province'] = merge_data['contents'].map(lambda s:' '.join(re.findall(keys,s)))\n",
    "\n",
    "d = defaultdict(lambda :'空',d)\n",
    "tokenizer = lambda line: [d[w] for w in line.split(' ')]\n",
    "tfv = TfidfVectorizer(tokenizer=tokenizer,norm=False, use_idf=False, smooth_idf=False, sublinear_tf=False)\n",
    "X_all_sp = tfv.fit_transform(merge_data['num_province'])\n",
    "sum_province = X_all_sp.toarray()\n",
    "for i in range(sum_province.shape[1]):\n",
    "    merge_data['sum_province_%d'%i] = sum_province[:,i]\n",
    "\n",
    "\n",
    "\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.mean([len(s.split(' ')) for s in lst]))\n",
    "merge_data['max_content_len'] = merge_data['uid'].map(length)\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.min([len(s.split(' ')) for s in lst]))\n",
    "merge_data['min_content_len'] = merge_data['uid'].map(length)\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.max([len(s.split(' ')) for s in lst]))\n",
    "merge_data['mean_content_len'] = merge_data['uid'].map(length)\n",
    "\n",
    "#merge_data['name_len'] = merge_data.name.map(lambda s:s if pd.isnull(s) else len(re.sub(r'[\\u4e00-\\u9fff]+','',s)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def num_missing(x):    \n",
    "    return sum(x.isnull())  \n",
    "\n",
    "merge_data['num_missing'] = merge_data.apply(num_missing, axis=1) \n",
    "\n",
    "#rank特征\n",
    "merge_data['rank_sum_content'] = merge_data['sum_content'].rank(method='max')\n",
    "merge_data['rank_sum_fans'] = merge_data['sum_fans'].rank(method='max')\n",
    "merge_data['rank_mean_retweet'] = merge_data['mean_retweet'].rank(method='max')\n",
    "merge_data['rank_mean_review'] = merge_data['mean_review'].rank(method='max')\n",
    "merge_data['rank_num_missing'] = merge_data['num_missing'].rank(method='max')\n",
    "\n",
    "\n",
    "#导入使用tfidf特征训练的模型的预测结果（采用stacking融合，把预测结果作为新特征加进模型）\n",
    "tfidf_stacking = pd.read_csv('../data/newfeat/stack_new.csv',encoding='UTF-8')\n",
    "merge_data = pd.concat([merge_data,tfidf_stacking],axis=1)\n",
    "\n",
    "#按小时划分，统计每个用户每3个小时发的微博数量\n",
    "#feat_time_1hour = pd.read_csv('./data/newfeat/feat_time_1hour.csv')\n",
    "#merge_data = pd.merge(merge_data,feat_time_1hour,on='uid',how='left')\n",
    "\n",
    "feat_time_3hour = pd.read_csv('../data/newfeat/feat_time_3hour.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,feat_time_3hour,on='uid',how='left')\n",
    "\n",
    "#导入使用word2vec特征训练的模型的预测结果\n",
    "'''w2v_stacking = pd.read_csv('../data/newfeat/w2v_prob1.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,w2v_stacking,on='uid',how='left')'''\n",
    "\n",
    "'''newmerge_feat1 = pd.read_csv('../data/newfeat/newmerge_feat.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,newmerge_feat1,on='uid',how='left')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_area1 = pd.read_csv('../data/newfeat/feat_area.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,feat_area1,on='uid',how='left')\n",
    "\n",
    "#########################################################################################\n",
    "cols = '|'.join(['twts_len','name_len','sum_province','sum_fans',\n",
    "                'age_','sex_','loc_',\n",
    "               'mean_retweet','sum_content','mean_review','num_missing',\n",
    "                 'w2v_f_prob','w2v_m_prob','w2v_young_prob','w2v_old_prob','w2v_mid_prob',\n",
    "                 'max_retweet','min_retweet','max_review','min_review',\n",
    "                 'rank_sum_content','rank_sum_fans','rank_mean_retweet','rank_mean_review','rank_num_missing',\n",
    "                 'timePeriod_3hour_0','timePeriod_3hour_1','timePeriod_3hour_2','timePeriod_3hour_3',\n",
    "                 'timePeriod_3hour_4','timePeriod_3hour_5','timePeriod_3hour_6','timePeriod_3hour_7',\n",
    "                 'name_isnull','image_isnull','fans_isnull','retweet_isnull','review_isnull',\n",
    "                 'area_0','area_1','area_2','area_3','area_4','area_5','area_6','area_7'\n",
    "                 ])\n",
    "cols = [c for c in merge_data.columns if re.match(cols,c)]\n",
    "\n",
    "age_le = LabelEncoder()\n",
    "ys = {}\n",
    "ys['age'] = age_le.fit_transform(merge_data.iloc[:3200]['age2'])\n",
    "\n",
    "loc_le = LabelEncoder()\n",
    "ys['loc'] = loc_le.fit_transform(merge_data.iloc[:3200]['location2'])\n",
    "\n",
    "sex_le = LabelEncoder()\n",
    "ys['sex'] = sex_le.fit_transform(merge_data.iloc[:3200]['sex'])\n",
    "\n",
    "\n",
    "merge_data = merge_data.fillna(0)\n",
    "task = ['sub']\n",
    "\n",
    "\n",
    "TR = 3200\n",
    "TE = 1240\n",
    "X_all = merge_data[cols]\n",
    "X = X_all[:TR]\n",
    "prds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "age\n",
      "====================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'age'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-a753abdbea88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'age'"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################\n",
    "#年龄预测部分\n",
    "label = 'age'\n",
    "print('='*20)\n",
    "print(label)\n",
    "print('='*20)\n",
    "y = ys[label]\n",
    "\n",
    "\n",
    "\n",
    "n_trees = 500\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"merror\",\n",
    "    \"num_class\":3,\n",
    "    'max_depth':8,\n",
    "    'min_child_weight':2.5,\n",
    "    'subsample':0.4,\n",
    "    'colsample_bytree':1,\n",
    "    'gamma':2.5,\n",
    "    \"eta\": 0.01,\n",
    "    \"lambda\":1,\n",
    "    'alpha':0,\n",
    "    \"silent\": 1,\n",
    "}\n",
    "if 'tr' in task:\n",
    "    for i,(tr,va) in enumerate(StratifiedKFold(y,n_folds=5)):\n",
    "        print('stack:%d/%d'%(i+1,5))\n",
    "        X_tr = X.iloc[tr]\n",
    "        y_tr = y[tr]\n",
    "        X_va = X.iloc[va]\n",
    "        y_va = y[va]\n",
    "        dtrain = xgb.DMatrix(X_tr, y_tr)\n",
    "        dvalid = xgb.DMatrix(X_va, y_va)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        bst1 = xgb.train(params, dtrain, 377, evals=watchlist,verbose_eval=20)\n",
    "        \n",
    "        \n",
    "if 'sub' in task:\n",
    "    dtrain = xgb.DMatrix(X, y)\n",
    "    dtest = xgb.DMatrix(X_all[TR:])\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    bst1 = xgb.train(params, dtrain, n_trees, evals=watchlist,\n",
    "                     verbose_eval=100)\n",
    "    prds.append(bst1.predict(dtest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "sex\n",
      "====================\n",
      "[0]\ttrain-error:0.124688\n",
      "[100]\ttrain-error:0.120937\n",
      "[200]\ttrain-error:0.118437\n",
      "[300]\ttrain-error:0.113438\n",
      "[400]\ttrain-error:0.107813\n",
      "[428]\ttrain-error:0.105625\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#性别预测部分\n",
    "label = 'sex'\n",
    "print('='*20)\n",
    "print(label)\n",
    "print('='*20)\n",
    "y = ys[label]\n",
    "\n",
    "\n",
    "\n",
    "n_trees = 429\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"error\",\n",
    "    'max_depth':4,\n",
    "    'min_child_weight':1.5,\n",
    "    'subsample':1,\n",
    "    'colsample_bytree':1,\n",
    "    'gamma':4,\n",
    "    \"eta\": 0.01,\n",
    "    \"lambda\":3,\n",
    "    'alpha':0,\n",
    "    \"silent\": 1,\n",
    "}\n",
    "if 'tr' in task:\n",
    "    for i,(tr,va) in enumerate(StratifiedKFold(y,n_folds=5)):\n",
    "        print('stack:%d/%d'%(i+1,5))\n",
    "        X_tr = X.iloc[tr]\n",
    "        y_tr = y[tr]\n",
    "        X_va = X.iloc[va]\n",
    "        y_va = y[va]\n",
    "        dtrain = xgb.DMatrix(X_tr, y_tr)\n",
    "        dvalid = xgb.DMatrix(X_va, y_va)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        bst2 = xgb.train(params, dtrain, 327, evals=watchlist,verbose_eval=20)\n",
    "        \n",
    "        \n",
    "if 'sub' in task:\n",
    "    dtrain = xgb.DMatrix(X, y)\n",
    "    dtest = xgb.DMatrix(X_all[TR:])\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    bst2 = xgb.train(params, dtrain, n_trees, evals=watchlist,\n",
    "                    verbose_eval=100)\n",
    "    _prd = bst2.predict(dtest)\n",
    "    prd = np.zeros((len(_prd),2))\n",
    "    prd[:,1] = _prd\n",
    "    prd[:,0] = 1 - prd[:,1]\n",
    "    prds.append(prd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "loc\n",
      "====================\n",
      "[0]\ttrain-merror:0.382812\n",
      "[100]\ttrain-merror:0.315\n",
      "[200]\ttrain-merror:0.297188\n",
      "[300]\ttrain-merror:0.279063\n",
      "[400]\ttrain-merror:0.260938\n",
      "[500]\ttrain-merror:0.24625\n",
      "[600]\ttrain-merror:0.234688\n",
      "[615]\ttrain-merror:0.23375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################\n",
    "#地区预测部分\n",
    "label = 'loc'\n",
    "print('='*20)\n",
    "print(label)\n",
    "print('='*20)\n",
    "y = ys[label]\n",
    "\n",
    "n_trees = 616\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"merror\",\n",
    "    \"num_class\":8,\n",
    "    'max_depth':5,\n",
    "    'min_child_weight':2.5,\n",
    "    'subsample':0.4,\n",
    "    'colsample_bytree':1,\n",
    "    'gamma':2.5,\n",
    "    \"eta\": 0.01,\n",
    "    \"lambda\":1,\n",
    "    'alpha':0,\n",
    "    \"silent\": 1,\n",
    "}\n",
    "if 'tr' in task:\n",
    "    for i,(tr,va) in enumerate(StratifiedKFold(y,n_folds=5)):\n",
    "        print('stack:%d/%d'%(i+1,5))\n",
    "        X_tr = X.iloc[tr]\n",
    "        y_tr = y[tr]\n",
    "        X_va = X.iloc[va]\n",
    "        y_va = y[va]\n",
    "        dtrain = xgb.DMatrix(X_tr, y_tr)\n",
    "        dvalid = xgb.DMatrix(X_va, y_va)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        bst3 = xgb.train(params, dtrain, 443, evals=watchlist,verbose_eval=20)\n",
    "       \n",
    "        \n",
    "if 'sub' in task:\n",
    "    dtrain = xgb.DMatrix(X, y)\n",
    "    dtest = xgb.DMatrix(X_all[TR:])\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    bst3 = xgb.train(params, dtrain, n_trees, evals=watchlist,\n",
    "                     verbose_eval=100)\n",
    "    prds.append(bst3.predict(dtest))\n",
    "\n",
    "\n",
    "#########################\n",
    "#生成提交结果\n",
    "if 'sub' in task:\n",
    "    sub = pd.DataFrame()\n",
    "    sub['uid'] = merge_data.iloc[TR:]['uid']\n",
    "    n = len(sub)\n",
    "    sub['gender'] = sex_le.inverse_transform(prds[1].argmax(axis=1))\n",
    "    sub.to_csv('../data/gender_sub.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "#location final result .py\n",
    "# In[133]:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "#读入训练测试数据，合并\n",
    "train_data = pd.read_csv('../data/train/train_labels.txt',sep=u'|',header=None).dropna(1)\n",
    "train_data.columns = ['uid','sex','age','location']\n",
    "test_data = pd.read_csv('../data/valid/valid_nolabel.txt',sep=u'|',header=None).dropna(1)\n",
    "test_data.columns = ['uid']\n",
    "total_data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "\n",
    "train_data_info = pd.read_csv('../data/train/train_info.txt',sep=u'|',header=None).dropna(1)\n",
    "train_data_info.columns = ['uid']\n",
    "train_data_info = train_data_info.drop_duplicates('uid')\n",
    "test_data_info = pd.read_csv('../data/valid/valid_info.txt',sep=u'|',header=None).dropna(1)\n",
    "test_data_info.columns = ['uid']\n",
    "test_data_info = test_data_info.drop_duplicates('uid')\n",
    "total_data_info = pd.concat([train_data_info,test_data_info],axis=0)\n",
    "\n",
    "\n",
    "#读入训练测试links数据，合并\n",
    "links = []\n",
    "for i, line in enumerate(open('../data/train/train_links.txt',encoding='UTF-8')):\n",
    "    line = line.split()\n",
    "    row = {'uid':int(line[0]),'sum_fans':len(line)-1,'fans':' '.join(line[1:])}\n",
    "    links.append(row)\n",
    "train_data_links = pd.DataFrame(links)\n",
    "train_data_links = train_data_links.drop_duplicates()\n",
    "\n",
    "\n",
    "links = []\n",
    "for i, line in enumerate(open('../data/valid/valid_links.txt',encoding='UTF-8')):\n",
    "    line = line.split()\n",
    "    row = {'uid':int(line[0]),'sum_fans':len(line)-1,'fans':' '.join(line[1:])}\n",
    "    links.append(row)\n",
    "test_data_links = pd.DataFrame(links)\n",
    "test_data_links = test_data_links.drop_duplicates()\n",
    "\n",
    "total_data_links = pd.concat([train_data_links,test_data_links],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#读入训练测试status数据，合并\n",
    "status = []\n",
    "for i, line in enumerate(open('../data/train/train_status.txt',encoding='UTF-8')):\n",
    "    \n",
    "    l = re.search(',',line).span()[0]\n",
    "    r = re.search(',',line).span()[1]\n",
    "    row = {'uid':int(line[:l]),'sta':line[r:]}\n",
    "    status.append(row)\n",
    "train_data_status = pd.DataFrame(status)\n",
    "\n",
    "\n",
    "status = []\n",
    "for i, line in enumerate(open('../data/valid/valid_status.txt',encoding='UTF-8')):\n",
    "    \n",
    "    l = re.search(',',line).span()[0]\n",
    "    r = re.search(',',line).span()[1]\n",
    "    row = {'uid':int(line[:l]),'sta':line[r:]}\n",
    "    status.append(row)\n",
    "test_data_status = pd.DataFrame(status)\n",
    "\n",
    "total_data_status = pd.concat([train_data_status,test_data_status],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#合并题目给的几个表数据\n",
    "merge_data = pd.merge(total_data,total_data_info,on='uid',how='left')\n",
    "merge_data = pd.merge(merge_data,total_data_links,on='uid',how='left')\n",
    "merge_data.index = range(len(merge_data))\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "\n",
    "total_data_status['retweet'] = total_data_status.sta.map(lambda s:int(s.split(',')[0]))\n",
    "total_data_status['review'] = total_data_status.sta.map(lambda s:int(s.split(',')[1]))\n",
    "total_data_status['source'] = total_data_status.sta.map(lambda s:s.split(',')[2])\n",
    "total_data_status['time'] = total_data_status.sta.map(lambda s:s.split(',')[3])\n",
    "total_data_status['content'] = total_data_status.sta.map(lambda s:','.join(s.split(',')[4:]))\n",
    "contents = total_data_status.groupby('uid')['content'].agg(lambda lst:' '.join(lst))\n",
    "merge_data['contents'] = merge_data.uid.map(contents)\n",
    "merge_data['sum_content'] = merge_data.uid.map(total_data_status.groupby('uid').size())\n",
    "\n",
    "\n",
    "\n",
    "#统计特征\n",
    "merge_data['max_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('max'))\n",
    "merge_data['max_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('max'))\n",
    "merge_data['min_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('min'))\n",
    "merge_data['min_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('min'))\n",
    "merge_data['median_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('median'))\n",
    "merge_data['median_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('median'))\n",
    "merge_data['mean_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('mean'))\n",
    "merge_data['mean_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('mean'))\n",
    "merge_data['std_retweet'] = merge_data.uid.map(total_data_status.groupby('uid')['retweet'].agg('std'))\n",
    "merge_data['std_review'] = merge_data.uid.map(total_data_status.groupby('uid')['review'].agg('std'))\n",
    "\n",
    "\n",
    "#location地区映射词表\n",
    "d = {'石家庄': '华北',\n",
    " '子陵庙': '华东',\n",
    " '深圳': '华南',\n",
    " '广州': '华南',\n",
    " '宝安': '华南',\n",
    " '刘庄': '华中',\n",
    " '沙市': '华中',\n",
    " '武汉': '华中',\n",
    " '襄阳': '华中',\n",
    " '安陆': '华中',\n",
    " '荆门': '华中',\n",
    " '西安': '西北',\n",
    " '银川': '西北',\n",
    " '成都': '西南',\n",
    " '绵阳': '西南',\n",
    " '上海': '华东',\n",
    " '云南': '西南',\n",
    " '内蒙古': '华北',\n",
    " '北京': '华北',\n",
    " '台湾': '华东',\n",
    " '吉林': '东北',\n",
    " '四川': '西南',\n",
    " '天津': '华北',\n",
    " '宁夏': '西北',\n",
    " '安徽': '华东',\n",
    " '山东': '华东',\n",
    " '山西': '华北',\n",
    " '辽宁': '东北',\n",
    " '重庆': '西南',\n",
    " '陕西': '西北',\n",
    " '青海': '西北',\n",
    " '香港': '华南',\n",
    " '黑龙江': '东北',\n",
    " '长白': '东北',\n",
    " '丹东': '东北',\n",
    " '大庸桥': '东北',\n",
    " '沈阳': '东北',\n",
    " '大连': '东北',\n",
    " '抚顺': '东北',\n",
    " '石家庄': '华北',\n",
    " '朝阳': '华北',\n",
    " '广东': '华南',\n",
    " '广西': '华南',\n",
    " '新疆': '西北',\n",
    " '江苏': '华东',\n",
    " '江西': '华东',\n",
    " '河北': '华北',\n",
    " '河南': '华中',\n",
    " '浙江': '华东',\n",
    " '海南': '华南',\n",
    " '湖北': '华中',\n",
    " '湖南': '华中',\n",
    " '澳门': '华南',\n",
    " '甘肃': '西北',\n",
    " '福建': '华东',\n",
    " '西藏': '西南',\n",
    " '贵州': '西南',\n",
    "}\n",
    "\n",
    "\n",
    "#将location和age转化成需要提交的范围\n",
    "def trans_loc(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    s = s.split(' ')[0]\n",
    "    if s == 'None':\n",
    "        return '华北'\n",
    "    if s == '海外':\n",
    "        return '境外'\n",
    "    return d[s]\n",
    "\n",
    "def trans_age(age):\n",
    "    if pd.isnull(age):\n",
    "        return age\n",
    "    if age <=1979:\n",
    "        return \"-1979\"\n",
    "    elif age<=1989:\n",
    "        return \"1980-1989\"\n",
    "    else:\n",
    "        return \"1990+\"\n",
    "\n",
    "\n",
    "\n",
    "merge_data['location2'] = merge_data['location'].map(trans_loc)\n",
    "merge_data['age2'] = merge_data['age'].map(trans_age)\n",
    "\n",
    "src_lst = total_data_status.groupby('uid')['source'].agg(lambda lst:' '.join(lst))\n",
    "merge_data['source_content'] = merge_data['uid'].map(src_lst) \n",
    "\n",
    "keys = '|'.join(d.keys())\n",
    "merge_data['source_province'] = merge_data['source_content'].map(lambda s:' '.join(re.findall(keys,s)))\n",
    "merge_data['num_province'] = merge_data['contents'].map(lambda s:' '.join(re.findall(keys,s)))\n",
    "\n",
    "d = defaultdict(lambda :'空',d)\n",
    "tokenizer = lambda line: [d[w] for w in line.split(' ')]\n",
    "tfv = TfidfVectorizer(tokenizer=tokenizer,norm=False, use_idf=False, smooth_idf=False, sublinear_tf=False)\n",
    "X_all_sp = tfv.fit_transform(merge_data['num_province'])\n",
    "sum_province = X_all_sp.toarray()\n",
    "for i in range(sum_province.shape[1]):\n",
    "    merge_data['sum_province_%d'%i] = sum_province[:,i]\n",
    "\n",
    "\n",
    "\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.mean([len(s.split(' ')) for s in lst]))\n",
    "merge_data['max_content_len'] = merge_data['uid'].map(length)\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.min([len(s.split(' ')) for s in lst]))\n",
    "merge_data['min_content_len'] = merge_data['uid'].map(length)\n",
    "length = total_data_status.groupby('uid')['content'].agg(lambda lst:np.max([len(s.split(' ')) for s in lst]))\n",
    "merge_data['mean_content_len'] = merge_data['uid'].map(length)\n",
    "\n",
    "#merge_data['name_len'] = merge_data.name.map(lambda s:s if pd.isnull(s) else len(re.sub(r'[\\u4e00-\\u9fff]+','',s)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def num_missing(x):    \n",
    "    return sum(x.isnull())  \n",
    "\n",
    "merge_data['num_missing'] = merge_data.apply(num_missing, axis=1) \n",
    "\n",
    "#rank特征\n",
    "merge_data['rank_sum_content'] = merge_data['sum_content'].rank(method='max')\n",
    "merge_data['rank_sum_fans'] = merge_data['sum_fans'].rank(method='max')\n",
    "merge_data['rank_mean_retweet'] = merge_data['mean_retweet'].rank(method='max')\n",
    "merge_data['rank_mean_review'] = merge_data['mean_review'].rank(method='max')\n",
    "merge_data['rank_num_missing'] = merge_data['num_missing'].rank(method='max')\n",
    "\n",
    "\n",
    "# In[134]:\n",
    "\n",
    "#导入使用tfidf特征训练的模型的预测结果（采用stacking融合，把预测结果作为新特征加进模型）\n",
    "tfidf_stacking = pd.read_csv('../data/newfeat/stack_new.csv',encoding='UTF-8')\n",
    "merge_data = pd.concat([merge_data,tfidf_stacking],axis=1)\n",
    "\n",
    "feat_time_3hour = pd.read_csv('../data/newfeat/feat_time_3hour.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,feat_time_3hour,on='uid',how='left')\n",
    "\n",
    "#导入使用word2vec特征训练的模型的预测结果\n",
    "'''\n",
    "w2v_stacking = pd.read_csv('./data/newfeat/w2v_prob1.csv')\n",
    "merge_data = pd.merge(merge_data,w2v_stacking,on='uid',how='left')\n",
    "'''\n",
    "\n",
    "'''newmerge_feat1 = pd.read_csv('../data/newfeat/newmerge_feat.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,newmerge_feat1,on='uid',how='left')'''\n",
    "\n",
    "'''feat_area1 = pd.read_csv('../data/newfeat/feat_area.csv',encoding='UTF-8')\n",
    "merge_data = pd.merge(merge_data,feat_area1,on='uid',how='left')'''\n",
    "\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "#########################################################################################\n",
    "cols = '|'.join(['twts_len','name_len','sum_province','sum_fans',\n",
    "                'age_','sex_','loc_',\n",
    "               'mean_retweet','sum_content','mean_review','num_missing',\n",
    "                 'w2v_f_prob','w2v_m_prob','w2v_young_prob','w2v_old_prob','w2v_mid_prob',\n",
    "                 'max_retweet','min_retweet','max_review','min_review',\n",
    "                 'rank_sum_content','rank_sum_fans','rank_mean_retweet','rank_mean_review','rank_num_missing',\n",
    "                 'timePeriod_3hour_0','timePeriod_3hour_1','timePeriod_3hour_2','timePeriod_3hour_3',\n",
    "                 'timePeriod_3hour_4','timePeriod_3hour_5','timePeriod_3hour_6','timePeriod_3hour_7',\n",
    "                 'name_isnull','image_isnull','fans_isnull','retweet_isnull','review_isnull',\n",
    "                 'area_0','area_1','area_2','area_3','area_4','area_5','area_6','area_7'\n",
    "                ])\n",
    "cols = [c for c in merge_data.columns if re.match(cols,c)]\n",
    "\n",
    "age_le = LabelEncoder()\n",
    "ys = {}\n",
    "#ys['age2'] = age_le.fit_transform(merge_data.iloc[:3200]['age2'])\n",
    "\n",
    "loc_le = LabelEncoder()\n",
    "ys['loc'] = loc_le.fit_transform(merge_data.iloc[:3200]['location2'])\n",
    "\n",
    "sex_le = LabelEncoder()\n",
    "ys['sex'] = sex_le.fit_transform(merge_data.iloc[:3200]['sex'])\n",
    "\n",
    "\n",
    "merge_data = merge_data.fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "loc\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# In[136]:\n",
    "\n",
    "task = ['tr']\n",
    "\n",
    "\n",
    "TR = 3200\n",
    "TE = 1240\n",
    "X_all = merge_data[cols]\n",
    "X = X_all[:TR]\n",
    "prds = []\n",
    "\n",
    "\n",
    "# In[137]:\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(2)\n",
    "X_poly=poly.fit_transform(X_all)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt=VarianceThreshold(0.001)\n",
    "X_poly=vt.fit_transform(X_poly)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "X_poly=ss.fit_transform(X_poly)\n",
    "\n",
    "\n",
    "# In[138]:\n",
    "\n",
    "X_poly=pd.DataFrame(X_poly)\n",
    "X_poly.columns='Poly_'+X_poly.columns.astype(str)\n",
    "\n",
    "\n",
    "# In[144]:\n",
    "\n",
    "X_train=X_poly[:TR]\n",
    "X_test=X_poly[TR:]\n",
    "\n",
    "\n",
    "# In[145]:\n",
    "\n",
    "#label=pd.read_csv('newlabel.csv',header=None,index_col=0)\n",
    "#label.columns=['uid','age','gender','province']\n",
    "\n",
    "\n",
    "# In[146]:\n",
    "\n",
    "merge_data.iloc[:3200]['location2'].value_counts()\n",
    "\n",
    "\n",
    "# In[416]:\n",
    "\n",
    "########################\n",
    "#地区预测部分\n",
    "label = 'loc'\n",
    "print('='*20)\n",
    "print(label)\n",
    "print('='*20)\n",
    "y = ys[label]\n",
    "dtrain=xgb.DMatrix(X_train,y)\n",
    "dtest=xgb.DMatrix(X_test)\n",
    "\n",
    "\n",
    "# In[424]:\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"merror\",\n",
    "    \"num_class\":8,\n",
    "    'max_depth':4,\n",
    "    #'min_child_weight':2.5,\n",
    "    'subsample':0.65,\n",
    "    'colsample_bytree':1.0,\n",
    "    'gamma':2.5,\n",
    "    \"eta\": 0.006,\n",
    "    #\"lambda\":1,\n",
    "    #'alpha':0,\n",
    "    \"silent\": 1,\n",
    "    #'seed':1123\n",
    "}\n",
    "xgb1=xgb.train(params,dtrain,num_boost_round=25)\n",
    "\n",
    "\n",
    "# In[425]:\n",
    "\n",
    "pre=xgb1.predict(dtest,ntree_limit=25)\n",
    "pre_loc=[loc_le.classes_[idx] for idx in pre.argmax(1)]\n",
    "sub = pd.DataFrame()\n",
    "sub['uid'] = merge_data.iloc[TR:]['uid']\n",
    "sub['province'] = pre_loc\n",
    "sub.to_csv('../data/location_sub.csv',index=False)\n",
    "\n",
    "# In[426]:\n",
    "\n",
    "#loc_pro2=pd.DataFrame(pre,columns=loc_le.classes_,index=test_data.uid)\n",
    "\n",
    "\n",
    "\n",
    "# In[427]:\n",
    "\n",
    "#loc_pro2.to_csv('loc_pro2.csv',header=True,index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
